# The model to fine-tune
model: "mlx-community/Llama-3.2-3B-Instruct-4bit"

# LoRA parameters
lora_parameters:
  rank: 8
  scale: 16.0
  dropout: 0.0

# Training parameters
optimizer: adamw
learning_rate: 1e-5  # Constant learning rate (Simpler, less prone to config errors)
num_layers: 16
batch_size: 1
iters: 200
steps_per_report: 10
steps_per_eval: 200
val_batches: 1
save_every: 100
adapter_path: "adapters"