# The model to fine-tune
model: "mlx-community/Llama-3.2-3B-Instruct-4bit"

# LoRA parameters
lora_parameters:
  rank: 16 # Efficient capacity for 200 examples
  scale: 32.0 # Standard strength (Alpha = 2x Rank)
  dropout: 0.05 # Prevents relying too much on specific neurons
  keys:
    - "mlp.gate_proj"
    - "mlp.up_proj"
    - "mlp.down_proj"
    - "self_attn.q_proj"
    - "self_attn.k_proj"
    - "self_attn.v_proj"
    - "self_attn.o_proj"

# Training parameters
optimizer: adamw
learning_rate: 2e-5
num_layers: 16
batch_size: 2
iters: 400 # ~4 Epochs (200 items / batch 2 = 100 steps per epoch)
steps_per_report: 10
steps_per_eval: 100 # Check validation every epoch
val_batches: 5 # Check more validation items to get a better read
save_every: 100
adapter_path: "adapters"

